# LLM Web Agent Configuration
# =============================================================================
# This file is loaded by default if present in current directory,
# ~/.config/llm-web-agent/config.yaml, or specified via --config
# =============================================================================

# Global Settings
debug: false
profile: false  # Enable performance profiling

# Browser Configuration
browser:
  # Engine: playwright (default) or selenium
  engine: playwright
  
  # Run in headless mode (true = no GUI, false = visible browser)
  headless: true
  
  # Browser viewport size
  viewport_width: 1280
  viewport_height: 720
  
  # Operation timeout in milliseconds
  timeout_ms: 30000
  
  # Browser channel: chromium (bundled), chrome, msedge
  # Options: chrome, chrome-beta, msedge, msedge-beta, msedge-dev, or null (for bundled chromium)
  browser_channel: null
  
  # Browser type (only for playwright): chromium, firefox, webkit
  browser_type: chromium
  
  # Custom user agent (optional)
  # user_agent: "Mozilla/5.0..."
  
  # Slow down operations by this amount (ms) - useful for debugging
  slow_mo: 0
  
  # Download settings
  # downloads_path: ./downloads
  accept_downloads: true
  
  # UI Overlay settings (visual feedback in browser)
  show_overlay: false
  highlight_elements: false
  overlay_position: right  # left, right
  highlight_color: "#FF6B6B"
  highlight_duration_ms: 1500

# LLM Provider Configuration
llm:
  # Provider: openai, anthropic, or copilot (via custom gateway)
  provider: openai
  
  # Model to use (Required)
  # Examples: gpt-4o, claude-3-opus-20240229
  model: gpt-4.1
  
  # API Base URL (Required for custom gateways like Copilot)
  # For OpenAI: https://api.openai.com/v1
  # For Copilot Gateway: http://127.0.0.1:3030
  base_url: http://localhost:3030
  
  # API Key is best set via environment variable: OPENAI_API_KEY
  # but can be set here if necessary
  api_key: sk-dummy
  
  # Sampling temperature (0.0 = deterministic, 1.0 = creative)
  temperature: 0.3
  
  # Maximum tokens for response
  max_tokens: 4096
  
  # Request timeout in seconds
  timeout: 60
  
  # Vision capabilities
  use_vision: true
  screenshot_quality: 80
  max_image_size: 1024

# Agent Behavior Configuration
agent:
  # Maximum steps allowed for a single task
  max_steps: 20
  
  # Number of retries for failed actions
  retry_attempts: 3
  
  # Delay between steps (milliseconds)
  step_delay_ms: 500
  
  # Error Handling
  stop_on_error: false
  continue_on_timeout: true
  
  # Screenshots
  screenshot_on_error: true
  screenshot_on_step: false
  
  # Logging
  verbose: false
  
  # Output settings
  output_dir: ./output
  save_trace: false
  
  # Pre-analysis (parallel LLM during browser startup)
  enable_pre_analysis: true
  pre_analysis_timeout_ms: 5000

# Logging Configuration
logging:
  # Log level: DEBUG, INFO, WARNING, ERROR, CRITICAL
  level: INFO
  
  # Log format string
  format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
  
  # Optional: Log to file
  # file: agent.log
  
  # Use JSON format for logs (easier for parsing)
  json_format: false
